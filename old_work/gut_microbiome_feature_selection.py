# -*- coding: utf-8 -*-
"""Gut_Microbiome_Feature_Selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDgVvMvRon52REglH4SyA-CVHz9ip7r-
"""

# Setup Environment
!pip install pandas numpy scikit-learn shap

"""# Gut Microbiome Feature Selection

Implementing species-level metagenomic feature selection using Bray-Curtis mapping, RFE, SHAP and Lasso.
"""

# Install libraries
import pandas as pd
import numpy as np
import os
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import shap

"""# Load and Preprocess GMrepo Data"""

# Define the directory path
data_path = "/content/drive/MyDrive/GM_Repo_Data"

# Load all files in to a dictionary
filenames = [
    "species_associated_with_D015212",
    "species_associated_with_D009765",
    "species_associated_with_D006262",
    "species_associated_with_D003924",
    "species_cooccurrence_in_D015179"
]

dataframes = {}

for file in filenames:
    full_path = os.path.join(data_path, file + ".tsv")
    try:
        df = pd.read_csv(full_path, sep="\t")
        dataframes[file] = df
        print(f"{file}: {df.shape} loaded.")
    except Exception as e:
        print(f"{file}: Failed to load – {e}")

# Preprocessing
def preprocess_species_df(df, min_abundance=0.001, drop_na_taxid=True):
    df_clean = df.copy()

    # Remove rows with missing taxon ID if column exists
    if drop_na_taxid and "NCBI taxon id" in df_clean.columns:
        df_clean = df_clean.dropna(subset=["NCBI taxon id"])

    # Convert abundance columns to numeric if exists
    for col in ["mean relative abundance", "median relative abundance", "SD of relative abundance"]:
        if col in df_clean.columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

    # Filter out low-abundance species if column exists
    if "mean relative abundance" in df_clean.columns:
        df_clean = df_clean[df_clean["mean relative abundance"] >= min_abundance]

    return df_clean.reset_index(drop=True)

# Apply preprocessing to files
species_data = {
  k: preprocess_species_df(v)
  for k, v in dataframes.items()
  if "species_associated_with" in k
}

# Add phenotype column if missing
for k, df in species_data.items():
    if "phenotype" not in df.columns:
        phenotype_code = k.split("_")[-1]
        df["phenotype"] = phenotype_code

# Print summary of data
for k, df in species_data.items():
    print(f"{k} → {df.shape[0]} species retained after filtering.")

required_cols = {"NCBI taxon id", "phenotype", "mean relative abundance"}

# Filter species_data
filtered_species_data = {
    k: df for k, df in species_data.items()
    if required_cols.issubset(df.columns)
}

# Check what columns each DataFrame contains
for k, df in species_data.items():
    print(f"{k}: columns = {list(df.columns)}")

# Combine all preprocessed, filtered data
if filtered_species_data:
    all_species_df = pd.concat(filtered_species_data.values(), ignore_index=True)

    # Pivot to get wide format: rows = taxon IDs, columns = phenotypes
    X = all_species_df.pivot_table(
        index="NCBI taxon id",
        columns="phenotype",
        values="mean relative abundance",
        fill_value=0
    ).T

    # Create target labels
    y = X.index.to_numpy()

    print("Data loaded and pivoted successfully.")
else:
    print("No valid dataframes to process after filtering.")

"""# Bray-Curtis Similarity Mapping"""

# Bray-Curtis distance matrix
bray_curtis_dist = squareform(pdist(X, metric='braycurtis'))



"""# Recursive Feature Elimination (RFE)"""

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(X.index)

# Model for RFE
model = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=model, n_features_to_select=20, verbose=1)
rfe.fit(X, y_encoded)

# Selected features
selected_features = X.columns[rfe.support_]
print("Selected Features:", selected_features)

# Around 50 - 180 features seems optimal
# IBS min 24
# Colorectal cancer ~ 184
# Rest in between

# If taxa removed, rescale total abundance = 1 or 100%
# Look in to standard scalar (especially for NN due to sensitivity)
# Represent all samples in a matrix, not per disease, not per class, for all data

"""# SHAP based feature importance"""

# Fit model
model.fit(X, y_encoded)

# SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# SHAP summary plot
shap.summary_plot(shap_values, X, plot_type="bar")

"""# Save the final feature list"""

# Save to CSV
selected_features.to_series().to_csv("selected_species_features.csv", index=False)